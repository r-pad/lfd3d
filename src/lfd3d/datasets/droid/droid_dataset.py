import json
import os
from glob import glob
from pathlib import Path

import numpy as np
import torch
import torch.nn.functional as F
import torchdatasets as td
from lfd3d.datasets.base_data import BaseDataModule, BaseDataset
from PIL import Image
from torchvision import transforms


class DroidDataset(BaseDataset):
    def __init__(self, root, dataset_cfg, split):
        super().__init__()
        self.root = root
        self.split = split

        self.cache_dir = dataset_cfg.cache_dir
        self.dataset_cfg = dataset_cfg

        self.droid_raw_dir = f"{root}/droid_raw"  # depth and metadata
        self.track_dir = f"{root}/droid_gripper_pcd"  # Gripper pcd rendered from Mujoco
        self.event_dir = f"{root}/droid_gemini_events"  # Subgoals and videos
        self.feat_dir = (
            f"{root}/droid_rgb_text_features"  # DINOv2 RGB/SIGLIP text features
        )

        self.current_dir = os.path.dirname(__file__)
        with open(f"{self.current_dir}/idx_to_fname_mapping.json") as f:
            self.idx_to_fname_mapping = json.load(f)

        self.num_points = dataset_cfg.num_points
        self.max_depth = dataset_cfg.max_depth
        self.captions = {}

        self.droid_index = self.load_split(split)
        self.droid_index = self.expand_all_events(self.droid_index)

        self.orig_shape = (180, 320)
        # Target shape of images (same as DINOv2)
        self.target_shape = 224
        self.rgb_preprocess = transforms.Compose(
            [
                transforms.Resize(
                    self.target_shape,
                    interpolation=transforms.InterpolationMode.BICUBIC,
                ),
                transforms.CenterCrop(self.target_shape),
            ]
        )
        self.depth_preprocess = transforms.Compose(
            [
                transforms.Resize(
                    self.target_shape,
                    interpolation=transforms.InterpolationMode.NEAREST,
                ),
                transforms.CenterCrop(self.target_shape),
            ]
        )

        with open(f"{self.current_dir}/zed_intrinsics.json") as f:
            self.camera_intrinsics = json.load(f)

        self.size = len(self.droid_index)
        # indexes of selected gripper points -> handpicked
        self.GRIPPER_IDX = np.array([356, 232, 16])

    def __len__(self):
        return self.size

    def load_camera_params(self, index):
        fname = self.idx_to_fname_mapping[index]
        metadata_file = glob(f"{self.droid_raw_dir}/1.0.1/{fname}/metadata*json")[0]
        with open(metadata_file) as f:
            metadata = json.load(f)

        # We work with camera 1 data across the dataset.
        cam_serial = metadata["ext1_cam_serial"]
        cam_params = self.camera_intrinsics[cam_serial]
        K = np.array(
            [
                [cam_params["fx"], 0.0, cam_params["cx"]],
                [0.0, cam_params["fy"], cam_params["cy"]],
                [0.0, 0.0, 1.0],
            ]
        )
        K = K / 4  # downsampled images
        K[2, 2] = 1
        baseline = cam_params["baseline"] / 1000
        return K, baseline

    def load_split(self, split):
        """
        Load the filenames corresponding to each split - [train, val, test]
        """
        with open(f"{self.current_dir}/{split}.json") as f:
            split_idxs = json.load(f)
        return split_idxs

    def expand_all_events(self, droid_index):
        """This function *expands* each event to have an associated chunk_idx.
        DROID events, generated by Gemini, typically have 3-4 chunks.
        If no chunks were generated due to bad data/other issues,
        the event is removed from the index.

        Updates `self.captions` with each event and its various subgoals

        Args:
            droid_index (list of int): List of indexes in DROID to be processed.

        Returns:
            expanded_droid_index (list of tuples (int, int)): A list of tuples
                  where each tuple contains the event index and the corresponding
                  chunk index.
        """
        expanded_droid_index = []
        for idx in droid_index:
            with open(f"{self.event_dir}/{idx}/subgoal.json") as f:
                subgoals = json.load(f)

            fname = self.idx_to_fname_mapping[idx]
            expanded_event_idx = [(idx, i) for i in range(len(subgoals))]
            expanded_event_caption = {
                (idx, i): subgoal for i, subgoal in enumerate(subgoals)
            }

            expanded_droid_index.extend(expanded_event_idx)
            self.captions.update(expanded_event_caption)
        return expanded_droid_index

    def load_rgbd(self, droid_idx, subgoal_idx, K, baseline):
        # Some pattern matching and string manipulation to get the image patch and its frame idx
        init_image_path = glob(f"{self.event_dir}/{droid_idx}/{subgoal_idx}*png")[0]
        init_frame_idx = int(
            os.path.basename(init_image_path).split("_")[1].split(".")[0]
        )
        end_image_path = glob(f"{self.event_dir}/{droid_idx}/{subgoal_idx + 1}*png")[0]
        end_frame_idx = int(
            os.path.basename(end_image_path).split("_")[1].split(".")[0]
        )

        # Return rgb/depth at beginning and end of event
        rgb_init = Image.open(init_image_path).convert("RGB")
        rgb_init = np.asarray(self.rgb_preprocess(rgb_init))
        rgb_end = Image.open(end_image_path).convert("RGB")
        rgb_end = np.asarray(self.rgb_preprocess(rgb_end))
        rgbs = np.array([rgb_init, rgb_end])

        fname = self.idx_to_fname_mapping[int(droid_idx)]
        disp_name = glob(f"{self.droid_raw_dir}/1.0.1/{fname}/*disp.npz")
        assert len(disp_name) == 1
        disp_name = disp_name[0]

        disparity = np.load(disp_name)["arr_0"].astype(np.float32)
        depths = np.divide(
            K[0, 0] * baseline,
            disparity,
            out=np.zeros_like(disparity),
            where=disparity != 0,
        )

        depth_init = Image.fromarray(depths[init_frame_idx])
        depth_init = np.asarray(self.depth_preprocess(depth_init))

        depth_end = Image.fromarray(depths[end_frame_idx])
        depth_end = np.asarray(self.depth_preprocess(depth_end))

        depths = np.array([depth_init, depth_end])
        return rgbs, depths, init_frame_idx, end_frame_idx

    def load_gripper_pcd(self, droid_idx, event_start_idx, event_end_idx):
        gripper_pcds = np.load(f"{self.track_dir}/{droid_idx}.npz")["arr_0"].astype(
            np.float32
        )
        start_tracks = gripper_pcds[event_start_idx]
        end_tracks = gripper_pcds[event_end_idx]
        return start_tracks, end_tracks

    def load_rgb_text_feat(self, rgb, droid_idx, event_idx):
        """
        Load RGB/text features generated with DINOv2 and SIGLIP
        """
        features = np.load(f"{self.feat_dir}/{droid_idx}/{event_idx}_compressed.npz")
        rgb_embed, text_embed = (
            features["rgb_embed"],
            features["text_embed"].astype(np.float32),
        )

        if self.dataset_cfg.rgb_feat:
            upscale_by = 4
            rgb_embed = rgb_embed.transpose(2, 0, 1)[None].astype(np.float32)
            rgb_embed = (
                F.interpolate(
                    torch.from_numpy(rgb_embed),
                    scale_factor=upscale_by,
                    mode="bilinear",
                    align_corners=False,
                )
                .numpy()
                .squeeze()
                .transpose(1, 2, 0)
            )
        else:
            # Just return the (normalized) RGB values if features are not required.
            rgb_embed = (((rgb / 255.0) * 2) - 1).astype(np.float32)
        return rgb_embed, text_embed

    def __getitem__(self, idx):
        index, subgoal_idx = self.droid_index[idx]

        K, baseline = self.load_camera_params(index)
        K_ = self.get_scaled_intrinsics(K, self.orig_shape, self.target_shape)

        subgoal = self.captions[(index, subgoal_idx)]
        caption = subgoal["subgoal"]
        start2end = torch.eye(4)  # Static camera in DROID
        fname = self.idx_to_fname_mapping[int(index)]

        # Note the use of K, not K_ for disparity -> depth conversion
        rgbs, depths, event_start_idx, event_end_idx = self.load_rgbd(
            index, subgoal_idx, K, baseline
        )

        start_tracks, end_tracks = self.load_gripper_pcd(
            index, event_start_idx, event_end_idx
        )

        rgb_embed, text_embed = self.load_rgb_text_feat(rgbs[0], index, subgoal_idx)
        start_scene_pcd, start_scene_feat_pcd, augment_tf = self.get_scene_pcd(
            rgb_embed, depths[0], K_, self.num_points, self.max_depth
        )

        action_pcd_mean, scene_pcd_std = self.get_normalize_mean_std(
            start_tracks, start_scene_pcd, self.dataset_cfg
        )
        start_tracks, end_tracks, start_scene_pcd = self.transform_pcds(
            start_tracks,
            end_tracks,
            start_scene_pcd,
            action_pcd_mean,
            scene_pcd_std,
            augment_tf,
        )

        # collate_pcd_fn handles batching of the point clouds
        item = {
            "action_pcd": start_tracks,
            "anchor_pcd": start_scene_pcd,
            "anchor_feat_pcd": start_scene_feat_pcd,
            "caption": caption,
            "text_embed": text_embed,
            "cross_displacement": end_tracks - start_tracks,
            "intrinsics": K_,
            "rgbs": rgbs,
            "depths": depths,
            "start2end": start2end,
            "vid_name": fname,
            "pcd_mean": action_pcd_mean,
            "pcd_std": scene_pcd_std,
            "gripper_idx": self.GRIPPER_IDX,
            "augment_R": augment_tf["R"],
            "augment_t": augment_tf["t"],
            "augment_C": augment_tf["C"],
        }
        return item


class DroidDataModule(BaseDataModule):
    def __init__(self, batch_size, val_batch_size, num_workers, dataset_cfg, seed):
        super().__init__(batch_size, val_batch_size, num_workers, dataset_cfg, seed)
        self.val_tags = ["robot"]

    def setup(self, stage: str = "fit"):
        self.stage = stage
        self.val_datasets = {}
        self.test_datasets = {}

        self.train_dataset = DroidDataset(self.root, self.dataset_cfg, "train")
        for tag in self.val_tags:
            self.val_datasets[tag] = DroidDataset(self.root, self.dataset_cfg, "val")
            self.test_datasets[tag] = DroidDataset(self.root, self.dataset_cfg, "test")

        if self.train_dataset.cache_dir:
            invalidating_cacher = td.cachers.ProbabilisticCacherWrapper(
                td.cachers.HDF5(Path(self.train_dataset.cache_dir)),
                invalidation_rate=self.dataset_cfg.cache_invalidation_rate,
                seed=self.seed,
            )
            self.train_dataset.cache(invalidating_cacher)
            for tag in self.val_tags:
                self.val_datasets[tag].cache(
                    td.cachers.HDF5(Path(self.train_dataset.cache_dir) / f"val_{tag}")
                )
