import json
import os
from glob import glob
from pathlib import Path

import numpy as np
import open3d as o3d
import pytorch_lightning as pl
import torch
import torch.nn.functional as F
import torch.utils.data as data
import torchdatasets as td
from PIL import Image
from torchvision import transforms

from lfd3d.utils.data_utils import collate_pcd_fn


class DroidDataset(td.Dataset):
    def __init__(self, root, dataset_cfg, split):
        super().__init__()
        self.root = root
        self.split = split

        self.cache_dir = dataset_cfg.cache_dir
        self.dataset_cfg = dataset_cfg

        self.droid_raw_dir = f"{root}/droid_raw"  # depth and metadata
        self.track_dir = f"{root}/droid_gripper_pcd"  # Gripper pcd rendered from Mujoco
        self.event_dir = f"{root}/droid_gemini_events"  # Subgoals and videos
        self.feat_dir = (
            f"{root}/droid_rgb_text_features"  # DINOv2 RGB/SIGLIP text features
        )

        self.current_dir = os.path.dirname(__file__)
        with open(f"{self.current_dir}/idx_to_fname_mapping.json") as f:
            self.idx_to_fname_mapping = json.load(f)

        # "Average" intrinsics for Zed (720 x 1080)
        K = np.array(
            [
                [522.42260742, 0.0, 653.9631958],
                [0.0, 522.42260742, 358.79196167],
                [0.0, 0.0, 1.0],
            ]
        )
        K = K / 4  # downsampled images
        K[2, 2] = 1
        self.K = K
        self.baseline = 0.120  # Baseline for ZED 2 is 120mm

        # Voxel size for downsampling
        self.voxel_size = 0.06
        self.captions = {}

        self.droid_index = self.load_split(split)
        self.droid_index = self.expand_all_events(self.droid_index)

        self.size = len(self.droid_index)

        self.orig_shape = (180, 320)
        # Target shape of images (same as DINOv2)
        self.target_shape = 224
        self.rgb_preprocess = transforms.Compose(
            [
                transforms.Resize(
                    self.target_shape,
                    interpolation=transforms.InterpolationMode.BICUBIC,
                ),
                transforms.CenterCrop(self.target_shape),
            ]
        )
        self.depth_preprocess = transforms.Compose(
            [
                transforms.Resize(
                    self.target_shape,
                    interpolation=transforms.InterpolationMode.NEAREST,
                ),
                transforms.CenterCrop(self.target_shape),
            ]
        )

        self.size = len(self.droid_index)

    def __len__(self):
        return self.size

    def load_split(self, split):
        """
        Load the filenames corresponding to each split - [train, val, test]
        """
        with open(f"{self.current_dir}/debug.txt", "r") as f:
            split_idxs = f.readlines()
        split_idxs = [int(i) for i in split_idxs]
        return split_idxs

    def expand_all_events(self, droid_index):
        """This function *expands* each event to have an associated chunk_idx.
        DROID events, generated by Gemini, typically have 3-4 chunks.
        If no chunks were generated due to bad data/other issues,
        the event is removed from the index.

        Updates `self.captions` with each event and its various subgoals

        Args:
            droid_index (list of int): List of indexes in DROID to be processed.

        Returns:
            expanded_droid_index (list of tuples (int, int)): A list of tuples
                  where each tuple contains the event index and the corresponding
                  chunk index.
        """
        expanded_droid_index = []
        for idx in droid_index:
            if not os.path.exists(f"{self.event_dir}/{idx}"):
                continue

            with open(f"{self.event_dir}/{idx}/subgoal.json") as f:
                subgoals = json.load(f)

            expanded_event_idx = [(idx, i) for i in range(len(subgoals))]
            expanded_event_caption = {
                (idx, i): subgoal for i, subgoal in enumerate(subgoals)
            }

            expanded_droid_index.extend(expanded_event_idx)
            self.captions.update(expanded_event_caption)
        return expanded_droid_index

    def get_scaled_intrinsics(self, K):
        # Getting scale factor from torchvision.transforms.Resize behaviour
        K_ = K.copy()

        scale_factor = self.target_shape / min(self.orig_shape)

        # Apply the scale factor to the intrinsics
        K_[0, 0] *= scale_factor  # fx
        K_[1, 1] *= scale_factor  # fy
        K_[0, 2] *= scale_factor  # cx
        K_[1, 2] *= scale_factor  # cy

        # Adjust the principal point (cx, cy) for the center crop
        crop_offset_x = (self.orig_shape[1] * scale_factor - self.target_shape) / 2
        crop_offset_y = (self.orig_shape[0] * scale_factor - self.target_shape) / 2

        # Adjust the principal point (cx, cy) for the center crop
        K_[0, 2] -= crop_offset_x  # Adjust cx for crop
        K_[1, 2] -= crop_offset_y  # Adjust cy for crop
        return K_

    def load_rgbd(self, droid_idx, subgoal_idx):
        # Some pattern matching and string manipulation to get the image patch and its frame idx
        init_image_path = glob(f"{self.event_dir}/{droid_idx}/{subgoal_idx}*png")[0]
        init_frame_idx = int(
            os.path.basename(init_image_path).split("_")[1].split(".")[0]
        )
        end_image_path = glob(f"{self.event_dir}/{droid_idx}/{subgoal_idx+1}*png")[0]
        end_frame_idx = int(
            os.path.basename(end_image_path).split("_")[1].split(".")[0]
        )

        # Return rgb/depth at beginning and end of event
        rgb_init = Image.open(init_image_path).convert("RGB")
        rgb_init = np.asarray(self.rgb_preprocess(rgb_init))
        rgb_end = Image.open(end_image_path).convert("RGB")
        rgb_end = np.asarray(self.rgb_preprocess(rgb_end))
        rgbs = np.array([rgb_init, rgb_end])

        fname = self.idx_to_fname_mapping[int(droid_idx)]
        disp_name = glob(f"{self.droid_raw_dir}/1.0.1/{fname}/*disp.npz")
        assert len(disp_name) == 1
        disp_name = disp_name[0]

        disparity = np.load(disp_name)["arr_0"].astype(np.float32)
        depths = np.divide(
            self.K[0, 0] * self.baseline,
            disparity,
            out=np.zeros_like(disparity),
            where=disparity != 0,
        )

        depth_init = Image.fromarray(depths[init_frame_idx])
        depth_init = np.asarray(self.depth_preprocess(depth_init))

        depth_end = Image.fromarray(depths[end_frame_idx])
        depth_end = np.asarray(self.depth_preprocess(depth_end))

        depths = np.array([depth_init, depth_end])
        return rgbs, depths, init_frame_idx, end_frame_idx

    def load_gripper_pcd(self, droid_idx, event_start_idx, event_end_idx):
        gripper_pcds = np.load(f"{self.track_dir}/{droid_idx}.npz")["arr_0"].astype(
            np.float32
        )
        start_tracks = gripper_pcds[event_start_idx]
        end_tracks = gripper_pcds[event_end_idx]
        return start_tracks, end_tracks

    def load_rgb_text_feat(self, droid_idx, event_idx):
        """
        Load RGB/text features generated with DINOv2 and SIGLIP
        """
        features = np.load(f"{self.feat_dir}/{droid_idx}/{event_idx}_compressed.npz")
        rgb_embed, text_embed = features["rgb_embed"], features["text_embed"]

        upscale_by = 4
        rgb_embed = rgb_embed.transpose(2, 0, 1)[None].astype(np.float32)
        rgb_embed = (
            F.interpolate(
                torch.from_numpy(rgb_embed),
                scale_factor=upscale_by,
                mode="bilinear",
                align_corners=False,
            )
            .numpy()
            .squeeze()
            .transpose(1, 2, 0)
        )
        return rgb_embed, text_embed

    def get_scene_pcd(self, rgb_embed, depth, K):
        height, width = depth.shape
        # Create pixel coordinate grid
        x = np.arange(width)
        y = np.arange(height)
        x_grid, y_grid = np.meshgrid(x, y)

        # Flatten grid coordinates and depth
        x_flat = x_grid.flatten()
        y_flat = y_grid.flatten()
        z_flat = depth.flatten()
        feat_flat = rgb_embed.reshape(-1, rgb_embed.shape[-1])

        # Remove points with invalid depth
        valid_depth = np.logical_and(z_flat > 0, z_flat < 5)
        x_flat = x_flat[valid_depth]
        y_flat = y_flat[valid_depth]
        z_flat = z_flat[valid_depth]
        feat_flat = feat_flat[valid_depth]

        # Create homogeneous pixel coordinates
        pixels = np.stack([x_flat, y_flat, np.ones_like(x_flat)], axis=0)

        # Unproject points using K inverse
        K_inv = np.linalg.inv(K)
        points = K_inv @ pixels
        points = points * z_flat
        points = points.T  # Shape: (N, 3)

        scene_pcd_o3d = o3d.geometry.PointCloud()
        scene_pcd_o3d.points = o3d.utility.Vector3dVector(points)
        scene_pcd_o3d_downsample = scene_pcd_o3d.voxel_down_sample(
            voxel_size=self.voxel_size
        )

        scene_pcd = np.asarray(scene_pcd_o3d_downsample.points)

        # Find closest indices in the original point cloud so we can index the features
        downsampled_indices = [
            np.argmin(np.linalg.norm(points - scene_pcd[i], axis=1))
            for i in range(scene_pcd.shape[0])
        ]
        scene_feat_pcd = feat_flat[downsampled_indices]
        return scene_pcd, scene_feat_pcd

    def __getitem__(self, idx):
        index, subgoal_idx = self.droid_index[idx]

        subgoal = self.captions[(index, subgoal_idx)]
        caption = subgoal["subgoal"]
        start2end = torch.eye(4)  # Static camera in DROID

        rgbs, depths, event_start_idx, event_end_idx = self.load_rgbd(
            index, subgoal_idx
        )

        start_tracks, end_tracks = self.load_gripper_pcd(
            index, event_start_idx, event_end_idx
        )

        rgb_embed, text_embed = self.load_rgb_text_feat(index, subgoal_idx)
        start_scene_pcd, start_scene_feat_pcd = self.get_scene_pcd(
            rgb_embed, depths[0], self.K
        )

        # Center on action_pcd
        action_pcd_mean = start_tracks.mean(axis=0)
        start_tracks = start_tracks - action_pcd_mean
        end_tracks = end_tracks - action_pcd_mean
        start_scene_pcd = start_scene_pcd - action_pcd_mean
        # Standardize on scene_pcd
        scene_pcd_std = start_scene_pcd.std(axis=0)
        start_tracks = start_tracks / scene_pcd_std
        end_tracks = end_tracks / scene_pcd_std
        start_scene_pcd = start_scene_pcd / scene_pcd_std

        # collate_pcd_fn handles batching of the point clouds
        item = {
            "action_pcd": start_tracks,
            "anchor_pcd": start_scene_pcd,
            "anchor_feat_pcd": start_scene_feat_pcd,
            "caption": caption,
            "text_embed": text_embed,
            "cross_displacement": end_tracks - start_tracks,
            "intrinsics": self.K,
            "rgbs": rgbs,
            "depths": depths,
            "start2end": start2end,
            "vid_name": index,  # no name in RT-1, just return idx in dataset
            "pcd_mean": action_pcd_mean,
            "pcd_std": scene_pcd_std,
        }
        return item


class DroidDataModule(pl.LightningDataModule):
    def __init__(self, batch_size, val_batch_size, num_workers, dataset_cfg):
        super().__init__()
        self.batch_size = batch_size
        self.val_batch_size = val_batch_size
        self.num_workers = num_workers
        self.stage = None
        self.dataset_cfg = dataset_cfg

        # setting root directory based on dataset type
        data_dir = os.path.expanduser(dataset_cfg.data_dir)
        self.root = data_dir

        # Subset of train to use for eval
        self.TRAIN_SUBSET_SIZE = 500

    def prepare_data(self) -> None:
        pass

    def setup(self, stage: str = "fit"):
        self.stage = stage

        self.train_dataset = DroidDataset(self.root, self.dataset_cfg, "train")
        if self.train_dataset.cache_dir:
            self.train_dataset.cache(
                td.cachers.Pickle(Path(self.train_dataset.cache_dir))
            )
        self.val_dataset = DroidDataset(self.root, self.dataset_cfg, "val")
        self.test_dataset = DroidDataset(self.root, self.dataset_cfg, "test")

    def train_dataloader(self):
        return data.DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True if self.stage == "fit" else False,
            num_workers=self.num_workers,
            collate_fn=collate_pcd_fn,
        )

    def train_subset_dataloader(self):
        """A subset of train used for eval."""
        indices = torch.randint(
            0, len(self.train_dataset), (self.TRAIN_SUBSET_SIZE,)
        ).tolist()
        return data.DataLoader(
            data.Subset(self.train_dataset, indices),
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=collate_pcd_fn,
            pin_memory=True,
        )

    def val_dataloader(self):
        val_dataloader = data.DataLoader(
            self.val_dataset,
            batch_size=self.val_batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=collate_pcd_fn,
        )
        return val_dataloader

    def test_dataloader(self):
        test_dataloader = data.DataLoader(
            self.test_dataset,
            batch_size=self.val_batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=collate_pcd_fn,
        )
        return test_dataloader
